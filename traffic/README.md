For all my tests I used these exact same "blocks" in my get_model function with a few tweaks to certain parts of the blocks. Each block consisted of a convolutional layer, pooling layer, and a dropout. I started with 3 of those blocks, starting with 64 filters on the first Conv2D and doubling the filters for another 2 blocks. Then flattening to a 1D vector to add dense layers. I used 2 dense layers, each followed by a dropout, first dropout was 50%, second was 30%. The neurons/units in my dense layers were 128 and 64 for the first and second layers respectively, but that changed later on with some testing to 512 and 256. The results from that change alone were significant. with the 128 and 64 units on the dense layers, I had an accuracy of 0.0533 and loss of 3.4924, when it was changed, the results went up to an accuracy of 0.9403 and loss of 0.2163 which was an impressive jump.

The final output layer of get_model was a dense layer with NUM_CATEGORIES number of neurons/units and a softmax activation, that was decided after some testing with other activation methods that just didnt work well with this case. It was after some research that I understood that softmax activation works best with classifications and categorization, while relu works best with learning patterns and predicting single continous values. The final addition that improved the results even more was adding batch normalization which improved performance and results to an accuracy of 0.9862 and loss of 0.0462. The batch normalization layers I used in the blocks has no specific configuration or custom parameters, but they helped improve the overall result so I kept them in.